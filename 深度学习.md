# 深度学习札记
--------------------------

1. **CNN**卷积网络
   - 神经网络作用：**特征提取**
   - CNN卷积网络：
       - CNN整体架构 ：**输入层** -> **卷积层** ->**非线性激活层**-> **池化层** -> **全连接层**
           1. **输入层** ：输入三维图像，即图像高度、图像宽度、颜色通道数(灰度只有一个通道，RGB图有三个通道)
           2. **卷积层** ：目的是对图像各个区域进行特征提取，最后得到一个**特征图**
               - 对于输入层的图像，利用**卷积核**滑动提取特征，且每个通道分别进卷积核，每个区域最后只对应一个特征值
               - 卷积核主要依赖**权重矩阵**和**偏置项**工作
               - 每个卷积核在每个区域最后对应的特征值等于**每个通道像素值矩阵和卷积核的权重矩阵的内积之和+偏置项**，这其实就是一次线性变换y=wx+b
                    - **卷积核的权重矩阵** ： 每个卷积核的具体权重由CNN训练过程决定，最开始被随机赋予最小值，不同核具有不同的权重矩阵，同一个卷积核在三个通道中的权重矩阵也不相同，相互独立，各自学习，单次卷积过程，卷积核的权重矩阵不变
                    - **偏置项**：随机设定的一个值，目的是调整阈值、提高灵活性、破坏对称型，也会随着CNN训练过程改变
               - 单次卷积，卷积核可以有**多个**，每个卷积核只对应一个特征图，多个卷积核意味着结果生成多个特征图
               - 卷积可以做**多次**，下一次卷积在上一次卷积得到的特征图基础之上
               - 卷积层常用参数：
                   - **滑动步长**：卷积核每次滑动的像素个数，步长越小提取的特征值越多，但是处理速度越慢
                   - **卷积核尺寸**：卷积核权重矩阵的大小，最小的卷积核一般是3*3大小
                   - **边缘填充**：输入图像的像素值矩阵中，越靠近中心的像素值被计算的次数越多，越边缘的像素值越容易被忽略，在原图像外围加上一圈0可以将原来的边界更靠近中心，从而使得每个位置的像素值最最后结果的影响更加公平
                   - **卷积核个数**：每个卷积核对应一个生成的特征图，其权重矩阵各自独立
               - 卷积结果长度/宽度计算公式：**H2 = (H1 - FH + 2P)/S + 1**   
                   - H2:卷积后的长度或宽度
                   - H1:卷积前的长度或宽度
                   - FH:卷积核的长度或宽度
                   - P:边缘厚度
                   - S:步长
           3. **非线性激活层**:
               - 线性函数：满足f(ax+by)=af(x)+bf(y)的函数，函数图像本身为直线
               - 为什么神经网络需要非线性激活函数？
                   - 非线性函数是为了增加神经网络非线性拟合能力，如线性函数y=ax+b，无论给这个函数增加多少次线性变换，都只能得到线性函数，无法得到非线性函数，也就意味着神经网络将无法拟合任何非线性函数，也就没有了存在的意义。
               - 非线性激活函数有哪些性质会导致出现问题？
                   - 如果非线性函数在正负无穷初梯度(可理解为导数)为0被称为**饱和函数**，如果同时梯度最大值较小，则在反向传播过程中会导致梯度消失
                   - 如果非线性激活函数是**非零均值函数**，即输出值的期望不为0，则会导致学习过程中所有参数符号始终一致，使得神经网络不易收敛
               - 常用的非线性激活函数：
                   - **Relu函数(修正线性单元)**：
                       - 公式: Relu = max(0,x)
                       - 当输入为x<0时，输出为0，当输入为x>0时，输出为x，这样对卷积后的特征图中的每一个特征值进行处理，
                       - 优势：非饱和函数，输入值大于零时梯度恒为1，解决回传梯度消失问题；同时计算简单，收敛较快
                       - 缺点：神经元死亡：当输入值小于0时，梯度为0；非零均值函数：影响收敛效果，可使用归一化解决；梯度爆炸：没有上界，梯度随计算累积至超过计算机数值上限，可使用参数初始化解决
                   - **LeakyReLU函数**：
                       - 公式：f(x)=x,(x>0 ); f(x)=ax,(x<=0)，其中a是一个很小的数，但不变
                       - 优势：具备ReLU的优点，，同时减轻神经元死亡的的问题
                       - 缺点：a是一个超参数，不能通过学习改变，但是对训练结果相当重要，使得影响训练结果的不确定因素增加
                   - **Prarametic ReLU函数**
                       - 公式：f(x)=x,(x>0 ); f(x)=ax,(x<=0)，其中a可以随着学习过程改变
                       - 优势：集合了LeakyReLU和ReLU的优势
                       - 缺点：增加模型复杂度与过拟合化风险
                   - **sigmoid函数**：
                       - 公式：f(x)=1/[1+e^(-x)],输出值在0到1之间
                       - 优势：可以直接表示概率，输出是一个平滑的概率分布
                       - 缺点：饱和函数：导致梯度消失；非零均值函数：导致不易收敛
                   - **双曲正切激活函数(Tanh)**：
                       - 公式：f(x)=(e^x - e^(-x))/(e^x + e^(-x))
                       - 优点：零均值函数：输出值以0为中心，使得权重更新时不会偏向任一方，梯度有正有负，有利于改善梯度流
                       - 缺点：饱和函数：当输入值的绝对值非常大时，仍然有梯度消失问题
           4. **池化层** ：目的是将来自于卷积层的特征图压缩(不含任何矩阵运算)
               - **最大池化**：只取所选区域中最大的特征值，特征值越大说明网络认为这个地方的特征越重要（只关注是否有，不关注哪里有）
               - **平均池化**：将所选区域的所有特征值求平均，作为新的特征值，此方法被证明显著弱于最大池化的效果
           5. **全连接层（FC）**：目的是对特征图进行维度上的改变，使用Softmax函数得出每个类别的概率
               - 