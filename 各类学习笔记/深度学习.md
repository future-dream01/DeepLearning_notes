# 深度学习笔记
--------------------------
1. **深度学习的历史** 
   1. 深度学习的三次浪潮：
      1. 第一次浪潮，控制论的兴起：线性神经元模型初步建立
         1. 1943年，神经科学家**麦卡洛克**和数学家**皮兹**发表论文《神经活动中内在思想的逻辑演算》，建立了神经元的数学模型，即**MCP模型**，将神经元的工作过程简化为三个过程：输入信号线性加权，求和，非线性激活（阈值法）
         2. 1957年，计算机科学家**罗森布拉特**发明**感知机**，这是由两层MCP神经元组成的神经网络，第一次将MCP用于机器学习，并成功运用**梯度下降**算法，感知机被证明可以收敛。
         3. 1960年，**Bernard Widrow**和学生**Hoff**发明了**自适应线性单元**(Adaline)和**多层自适应线性单元**(Madaline),Madaline中包含多个Adaline，是早期的多层神经网络,可以对输入数据进行二分类，同时使用了**最小均方误差**损失函数和**随机梯度下降算法**更新权重，这极大地引起了科学家对人工神经网络的兴趣，但其本质上仍为线性模型，无法处理异或问题
         4. 1969年，”神经网络之父“**明斯基**和**佩珀特**共同编写《Perceptrons: an introduction to computational geometry》，证明了单层感知机无法解决线性不可分问题，同时发现了当时神经网络面临的死局：（1）基本感知机无法处理异或回路。（2）复杂多层感知机以当时计算机的计算能力无法完成训练。基本杀死感知机方向，使得深度学习浪潮迎来了第一次大衰退。但是这启发了后续神经网络的发展方向：提高硬件算力、加入更多的层和引入非线性计算。
      2. 第二次浪潮：联结主义潮流：基本思想是网络通过将大量计算单元连接在一起可以实现智能行为，此时多层神经网络开始出现，但没有较好的训练方法，也没有足够的训练数据
         1. 1974年，**Paul Werbos**在博士论文《Beyond regression : new tools for prediction and analysis in the behavioral sciences》中首次提出**反向传播**算法，使得训练多层神经网络成为可能。
         2. 1980年**福岛邦彦**发明**神经认知机**，成为了后来卷积神经网络的基础
         3. 1983年，AI教父**辛顿**发明玻尔兹曼机，为深度学习和无监督学习提供理论基础
         4. 1986年，深度学习(DeepLearning)一次首次由Rina Dechter在论文《LEARNING WHILE SEARCHING IN CONSTRAINT-SATISFACTION-PROBLEMS》中引入机器学习社区，
         5. 1989年，**杨立昆**(Yann LeCun)在《Backpropagation Applied to Handwritten Zip Code Recognition》中提出一种用反向传播进行更新的卷积神经网络，称为 **LeNet** ，启发了后续深度卷积网络的发展
         6. 20世纪90年代统计学习登场，与之而来的还有**支持向量机**、**核方法**、**图模型**，同时神经网络方面的研究不能满足市场不切实际的要求，深度学习发展浪潮迎来第二次衰退
      3. 第三次浪潮：真正的深度学习成为可能：深层神经网络的训练方法和训练所需数据量均得到突破
         1.  2006年，**辛顿**和同事提出**深度信念网络(DBN)**，将由多层受限玻尔兹曼机构成的DBN通过**贪婪逐层预训练**的方式解决了深层网络结构的训练难题，该方法很快被证明在其他网络的训练上同样有效。
         2.  2012年，**辛顿**的学生**Alex Krizhevsky**，**Ilya Sutskever** 和 **Geoffrey Hinton**，参加ImageNet图像分类竞赛，使用**AlexNet**，花费 5 到 6 天，采用 2 块 NVIDIA GTX 580 3GB GPUs 完成了模型训练并以惊人优势拿下冠军，标志着深度卷积神经网络正式走向大众视野
2. **深度学习零碎知识**
   1. **batch**: 批次，在卷积网络中通常是以一个批次为单位进入输入层，一个批次通常包含多个样本(图片)，甚至是不同类别的样本，决定一个批次中包含多少张图片的参数是一个超参数，训练前人为规定
   2. **epoch**：周期，一个周期表示所有样本都经过一次前向传播和反向传播，即遍历了一次样本集，周期数决定总共遍历多少次，也是超参数，人为决定
   3. **训练集**：模型前向传播和反向传播直接使用的数据集；**验证集**：训练过程中每一轮训练之后评估当前轮次训练结果的数据集，用来执行早期停止、防止过拟合；**测试集**：模型训练完毕之后用来评估整个模型性能的数据集
   4. TensorRT加速：对于推理过程，可以将pytorch格式的网络转化为TensorRT格式的网络，TensorRT格式通过各种方式（层融合、精度校准、内核优化）提高模型在特定硬件上的推理速度，特别适用于边缘设备上的推理过程，在安装过TensorRT、torch2trt后可以将模型转化成.engine文件，在yolox中使用--trt命令行参数即可使用.engine文件加速推理
   5. 计算机视觉的三大基本任务：
        1. 目标分类
           1. 从AlexNet
        2. 目标检测
           1. 
        3. 目标分割：像素级别的目标识别
           1. 语义分割：不区分属于相同类别的不同实例
              1. 主要是使用FCN（Fully Convolutional Networks）
              2. Unet（基于FCN）
           2. 实例分割：区分相同类别的不同实例
              1. Mask RCNN
   6. 硬件的算力往往与使用的数据格式相关：
        1. **FP32**：标准单精度浮点格式，32位，计量单位**TFLOPS(每秒执行的浮点运算次数 万亿次)**，在精度和占用的储存空间之间实现了很好的平衡
        2. **TF32**：NVIDIA推出的浮点格式，32位，计量单位**TFLOPS**精度与FP32相同，但做了优化，可以提高性能
        3. **FP16**:半精度浮点格式，16位，计量单位**TFLOPS**在精度即范围上不及FP32，但可以减少存储需求和计算时间，常用于加速推理
        4. **BF16**：GOOGLE提出的一种数据格式，16位，指数位与FP32相同，所以表示范围相同，但是尾数位不及FP32，牺牲精度的同时加速了运算
        5. **Int8**：8位整数格式 计量单位**TOPS(每秒进行的运算次数 万亿次)**，精度小，范围小，但是在资源受限的边缘计算领域广泛用于运算加速
   7. 深度神经网络的训练过程其实就是**端到端**的学习过程，即，我们只知道输入端和输出端的信息，对于中间的过程我们无从知晓
   8. **迁移学习**（Transfer Learning）和**知识蒸馏**（Knowledge Distillation）：
      1. 迁移学习：是一种机器学习方法，它使得一个在某个任务上训练好的模型能够被重新利用在另一个相关但不同的任务上，这种方法的核心思想是，**即使两个任务不完全相同，它们之间也可能共享一些知识或模式。**通过迁移这些知识，可以加速新任务的学习过程，甚至在数据较少的情况下也能达到较好的性能。迁移学习在深度学习领域尤为重要，因为**深度学习模型通常需要大量的数据和计算资源才能从头开始训练。**
      2. 知识蒸馏：主要目的是将一个大型、复杂的模型（通常称为“教师模型”）的知识转移到一个更小、更高效的模型（称为“学生模型”）
      3. 区别：迁移学习旨在将知识从一个任务转移到另一个任务，常用于处理数据量较小的新任务；知识蒸馏旨在将知识从同一任务中的一个较大模型转移到一个较小模型，用于模型压缩。
   9. **强化学习**：
   10. 深度学习中一般把整个模型从小到大层分为：**神经元**——>**层**——>**块**——>**模型(局部)**——>**模型(整体)**
   11. 深度学习中，张量是数据的基本形式，对于图像数据，其一般格式为[批次数，通道数，高度，宽度]，也就是说输入每个层的数据其实并不是单个图片，而是一整个批次的图像被整合在一个张量中被输入进一个层，这有利于GPU进行并行计算，但注意：并行计算并不把同一批次中不同图片的特征混合在同一个层中计算，而是保持独立性，即其实是有数量与批次数相同、相互之间参数的平行层分别计算每一张独立的图片，最后再整合进一个张量中。
   12. 模型性能的衡量：
       1.  **泛化能力**：是指机器学习模型对未见过的新数据（即不包含在训练集中的数据）的处理能力，这意味着模型在训练集上学习到了正确的规律，这些规律在新数据上依然有用；
       2.  **过拟合**：模型过度学习了训练数据中的特性，包括噪声和非代表性的特征，导致其泛化能力下降，使得其在训练集以外的数据上表现较差；
       3.  **如何提高模型的泛化能力**：
           1.  数据增强*：通过增加训练数据的多样性，例如通过旋转、缩放、裁剪或颜色调整来扩展图像数据集
           2.  正则化：使用如L1或L2正则化来限制模型复杂度，减少过拟合的风险
           3.  交叉验证：使用交叉验证来评估模型在新数据上的表现
           4.  早停：在训练过程中，一旦在验证集上的性能开始下降，即停止训练，以防过拟合
           5.  简化模型结构：减少模型的复杂性，例如减少层数或参数数量，以避免学习过于复杂的模式
           6.  集成方法：如随机森林或梯度提升机等集成学习方法，可以通过组合多个模型来提高泛化能力
   13. 图像数据的表示方式一般有两种：
       1.  一般的图像处理中：[高度，宽度，通道数]
       2.  深度学习模型中：[批次数，高度，宽度，通道数]

3. **pytorch**
   1. nn: 神经网络模块
      1. nn.Model:模型父类，一般自己定义的子类需要继承此父类
      2. 模块中预定义的卷积层：
         1. nn.Conv1d(输入通道数，输出通道数，卷积核尺寸): 一维卷积层，适用于处理序列数据，如时间序列或文本数据。
         2. nn.Conv2d: 二维卷积层，最常用于处理图像数据。
         3. nn.Conv3d: 三维卷积层，用于处理体积数据，如医学图像或视频数据。
         4. nn.ConvTranspose1d: 一维转置卷积层，也称为反卷积层，用于对一维数据进行上采样。
         5. nn.ConvTranspose2d: 二维转置卷积层，用于图像数据的上采样，常见于生成对抗网络（GANs）
         6. nn.ConvTranspose3d: 三维转置卷积层，用于三维数据的上采样。
         7. nn.DepthwiseConv2d: 深度可分离二维卷积层，其中每个输入通道被单独卷积。
         8. nn.GroupConv2d: 分组卷积层，它将输入和输出通道分成多组，以减少参数数量和计算量。
4. **CNN**卷积网络
   1. CNN卷积网络：
       - CNN整体架构 ：**输入层** >>> **卷积层** >>>**批量归一化层**>>>**非线性激活层**>>> **池化层** >>>**展平层** >>**全连接层**
           1. **输入层** ：输入四维张量，即(**批次数，通道数，图像高度，图像宽度**)(灰度只有一个通道，RGB图有三个通道)
              - 输入层的图片大小必须统一，例如yolov5默认的输入图片大小是640*640，如果遇到大小不相同的情况，则会先按照长宽比进行缩放，使得短边长度等于640，然后再对图片空白区域进行填充，一般填充黑色，这中处理方案的好处在于保持原始图片的长宽比，不会使得图像被拉伸变形 
              - 输入层通常是一个batch，即一个批次，包含很多张图片甚至很多个类别的图片，但是这些图片是**分别、独立**进入后面的所有层，而不是同时进入，所有图片的汇总发生在损失函数计算时的每个图片损失值的求和
           2. **卷积层** ：目的是对图像各个区域进行特征提取，最后得到一个或多个**特征图**
               - 对于输入层的图像，利用**卷积核**滑动提取特征，且每个通道分别进卷积核，每个区域最后只对应一个特征值，卷积核的每次卷积操作都对应着一个神经元，每次的仿射变换得出最后的特征值都是一次神经元输出，所以一个卷积核对应多个**神经元**，每个神经元可以对应多个权重矩阵(数量等于上层输入的深度)，只不过在卷积层中这些神经元之间共享参数。
               - 卷积核: 
                  1. 卷积核主要依赖**权重矩阵**和**偏置项**工作
                     - 每个卷积核在每个区域最后对应的特征值等于**每个通道像素值矩阵和卷积核的权重矩阵的内积之和+偏置项**，这其实就是多个仿射变换(线性变换+平移变换)y=wx+b叠加。
                     - **卷积核的权重矩阵** ：相当于仿射变换中的w，每个卷积核的具体权重由CNN训练过程决定，最开始被随机赋值，不同核具有不同的权重矩阵，同一个卷积核在三个通道中的权重矩阵也不相同，相互独立，各自学习，卷积核前向传播的过程，卷积核的权重矩阵不变
                     - **偏置项**：相当于仿射变换中的b，随机设定的一个值，目的是调整阈值、提高灵活性、破坏对称型，也会随着CNN训练过程改变
                  2. **卷积核的深度**：卷积核的深度与输入的数据深度相同，如：输入是一个三通道的彩色图像，那么对应卷积核的深度就必须是3；如果输入的是多维特征图，即多张特征图，则卷积核的深度也应当与之相对应，其中各个维度之间的权重矩阵并不相同，但是**同一通道的权重矩阵所有神经元中共享**。
                  3. **卷积核的个数**：一个卷积层中卷积核可以有多个，一个卷积核对应一个生成的特征图，卷积核的个数和最后特征图的深度相同
                  4. **卷积核的大小**:同一个卷积层中的所有卷积核大小相同
                  5. 卷积可以做**多次**，下一次卷积在上一次卷积得到的特征图基础之上
               - 卷积层常用参数：
                  1. **滑动步长**：卷积核每次滑动的像素个数，步长越小提取的特征值越多，但是处理速度越慢
                  2. **卷积核尺寸**：卷积核权重矩阵的大小，最小的卷积核一般是3*3大小
                  3. **边缘填充**：输入图像的像素值矩阵中，越靠近中心的像素值被计算的次数越多，越边缘的像素值越容易被忽略，在原图像外围加上一圈0可以将原来的边界更靠近中心，从而使得每个位置的像素值最最后结果的影响更加公平
                  4. **卷积核个数**：每个卷积核对应一个生成的特征图，其权重矩阵各自独立
               - 卷积结果长度/宽度计算公式：$$H2 = \frac{H1 - FH + 2P}{S} + 1$$ 
                   - H2:卷积后的长度或宽度
                   - H1:卷积前的长度或宽度
                   - FH:卷积核的长度或宽度
                   - P:边缘厚度
                   - S:步长
           3. **批量归一化层**：目的是使得每一层的输入都有一个相似分布的输入，防止一次正向传播和反向传播之后，参数更新使得靠近输出层的区域神经元的输入因为之前所有神经元所有输出抖动的累积发生剧烈抖动，导致损失函数难以收敛
              1. 实现过程：
                 1. 对于此批次所有样本相同特征图中的所有特征值，先计算所有数值的均值：$$\mu =\frac{1}{m}\sum\limits_{i=1}^{m}x_i$$ 其中m是此特征图中特征值个数，$x_i$是每个特征值
                 2. 然后计算这些数值的方差：$$\sigma^2=\frac{1}{m}\sum\limits_{i=1}^{m}(x_i-\mu)^2$$ 
                 3. 然后对每个$x_i$元素标准化：$${x_i}^{'}=\frac{x_i-\mu}{\sqrt{\sigma^2+\delta}}$$ 其中$\delta$是无穷小量，防止分母变成0
                 4. 然后对每个${x_i}^{'}$批量归一化: $$y_i= \gamma*{x_i}^{'}+\beta$$ 其中$\gamma$：缩放参数、$\beta$：平移参数 是可学习参数
              2. 批量归一化操作中，计算一次均值和方差实际上是使用了一个批次(顾名思义)中**所有样本的同一特征图**（或者说同一通道）上的所有特征值，这样其实并没有将多个样本的信息混合在一起，因为批量归一化操作只是改变了数据的分布，使其更加统一、均匀，具有相同的均值和方差，并没有改变每个样本每个数据之间相对的特征
           4. **非线性激活层**:增加网络对非线性函数的拟合能力
               - 线性函数：满足$f(ax+by)=af(x)+bf(y)$的函数，函数图像本身为直线
               - 为什么神经网络需要非线性激活函数？
                   - 非线性函数是为了增加神经网络非线性拟合能力，如线性函数y=ax+b，无论给这个函数增加多少次仿射变换， 都只能得到线性函数，无法得到非线性函数，也就意味着神经网络将无法拟合任何非线性函数，也就几乎没有了存在的意义。
                   - 然而一旦引入了非线性函数，此处以reLU函数为例，他在神经网络中的作用方式主要有两种：一种是前向传播过程中Relu函数和仿射变换的**多层嵌套**：$output = ReLU(w2 * ReLU(w1 * x + b1) + b2)$,这种作用形式可以创造出单尖点的非线性函数，但是显然远远不够，在卷积核中运行内积算法时所以在全连接层中，会出现将这一层中所有神经元输出相加的过程，由于每个神经元的输出与最初的输入都是一个非线性关系，所有非线性函数在这一层**相互叠加**，即出现了 $ReLU(w1*x + b1) + ReLU(w2*x + b2)+……$ 的现象，而这样的相加则直接产生尖点无限多的复杂非线性函数，完全达到拟合所有非线性函数的效果，在CNN中，每个神经元的输出都要套上一个ReLU激活函数，以最大程度提高其非线性函数的拟合度
               - 非线性激活函数有哪些性质会导致出现问题？
                   - 如果非线性函数在正负无穷初梯度(可理解为导数)为0被称为**饱和函数**，如果同时梯度最大值较小，则在反向传播过程中会导致梯度消失
                   - 如果非线性激活函数是**非零均值函数**，即输出值的期望不为0，则会导致学习过程中所有参数符号始终一致，使得神经网络不易收敛
               - 常用的非线性激活函数：
                   - **ReLU函数(线性修正单元)**：
                       - 公式: $$ReLU = max(0,x)$$
                         - 其中当输入为x<=0时，输出为0，当输入为x>0时，输出为x，这样对卷积后的特征图中的每一个特征值进行处理，
                       - 优势：非饱和函数，输入值大于零时梯度恒为1，解决回传梯度消失问题；同时函数简单，收敛较快，这一点也使得Relu函数称为卷积网络中使用最多的激活函数
                       - 缺点：神经元死亡：当输入值小于0时，梯度为0；非零均值函数：影响收敛效果，可使用归一化解决；梯度爆炸：没有上界，梯度随计算累积至超过计算机数值上限，可使用参数初始化解决
                   - **LeakyReLU函数**：
                       - $$
                       f(x) = \begin{cases}
                       x & \text{if} x > 0 \\
                       ax & \text{if} x \leq 0 
                       \end{cases}
                       $$
                         - 其中a是一个很小的数，但不变
                       - 优势：具备ReLU的优点，，同时减轻神经元死亡的的问题
                       - 缺点：a是一个超参数，不能通过学习改变，但是对训练结果相当重要，使得影响训练结果的不确定因素增加
                   - **Prarametic ReLU函数**
                       - $$
                       f(x) = \begin{cases}
                       x & \text{if} x > 0 \\
                       ax & \text{if} x \leq 0 
                       \end{cases}
                       $$
                         - 其中a可以随着学习过程改变
                       - 优势：集合了LeakyReLU和ReLU的优势
                       - 缺点：增加模型复杂度与过拟合化风险
                   - **sigmoid函数**：
                       - 公式：$$f(x)=\frac{1}{1+e^{-x}}$$
                         - 输出值在0到1之间
                       - 优势：可以直接表示概率，输出是一个平滑的概率分布
                       - 缺点：饱和函数：导致梯度消失；非零均值函数：导致不易收敛
                   - **SiLU函数**：
                       - 公式：$$SiLU=x*\frac{1}{1+e^{-x}}$$
                       - 将特征值和经过sigmoid函数之后得到的值相乘来引入非线性，非常类似**特征重塑**操作，给与激活函数动态调整特征值所占权重的能力
                   - **双曲正切激活函数(Tanh)**：
                       - 公式：$$f(x)=\frac{e^x - e^{-x}}{e^x + e^{-x}}$$
                       - 优点：零均值函数：输出值以0为中心，使得权重更新时不会偏向任一方，梯度有正有负，有利于改善梯度流
                       - 缺点：饱和函数：当输入值的绝对值非常大时，仍然有梯度消失问题
           5. **池化层** ：目的是将来自于卷积层的特征图压缩(不含任何矩阵运算)
               - **最大池化**：只取所选区域中最大的特征值，特征值越大说明网络认为这个地方的特征越重要（只关注是否有，不关注哪里有）
               - **平均池化**：将所选区域的所有特征值求平均，作为新的特征值，此方法被证明显著弱于最大池化的效果
           6. **展平层**：目的是将池化层输出的多维度特征图转变为全连接层输入需要的一维向量
              1. 在一般的卷积神经网络中此过程只是简单的将多位特征图中的特征值拼接成一维向量，并不涉及参数学习和数值的改变
           7. **全连接层（FC）**：(分类任务网络)目的：使用Softmax函数得出样本属于每个类别的概率
               1. **全连接层中的前几层神经元**：
                  1. 池化层的输出特征图经过展平层从二维矩阵变成了一维列向量，每一层中的每个神经元中包含一个一维行向量，存放权重系数，同一层中所有神经元的权重向量合在一起组成了权重系数矩阵，同时还有一个偏置列向量，存放偏置的值，全连接层的仿射变换过程就是**全部神经元权重系数行向量组成的矩阵和输入的特征值列向量进行矩阵乘法，然后将每个神经元对应的结果(其实就是每个权重系数分别都乘以列向量每一位上的特征值再相加得到的值，有点像行向量和列向量的内积)再加上偏置矩阵中相应位置的值即可($y=kx+b$)。**仿射变换的结果是得到一个列向量(基本的矩阵乘法知识)，作为下一层全连接神经元的输入，以此类推。需要注意的是每一层每个神经元的权重系数行向量长度和这一层输入的列向量长度相等，而每一层的总神经元数量又对应了这一层输出列向量长度
                  2. 经过仿射变换得到的列向量还要经过一层激活函数，通过激活函数**嵌套**增强非线性拟合能力，同时内积的方式也做到了通过**叠加**的方式将多个简单非线性函数组合成复杂非线性函数，很类似于卷积层中实际进行的操作
                  3. 不难发现，全连接层中的每一个神经元都与上一层中所有神经元输出均相连，而与此不同的是卷积层中神经元均与上层中的部分神经元相连，全连接层因此得名。
               2. **全连接层中的最后一层神经元**：
                  1. 在分类问题中，softmax函数的输入需要是一个长度与类别数相等的向量，其中向量中的每一项都表示原图像经过一系列操作后的对于每个类别的得分，所以最后一层神经元的个数必须等于总类别数，这样才能保证这一层输出的列向量长度等于类总数，以提供给Softmax函数作为数据源。
                  2. 不难发现，对于全连接层最后一层中的每个神经元，其实都是对之前的所有网络结构中的所有非线性拟合的一个汇总，之前过程中所有的仿射变换、激活函数嵌套、激活函数叠加，均汇总到了最后一层的每个神经元上，因此这里代表每个类别的每个神经元的输出都是最后每个类别的拟合结果。
               3. Softmax概率预测函数：
                  1. 作用：将每个类别的得分转换为概率，所有类别概率之和为1
                  2. 公式：
                     - $$Softmax(z_i)= \frac{e^{z_i}}{\sum\limits_{j=1}^{K}e^{z_j}}$$
                     - 原向量Z每一项包含每个类别得分，经过Softmax函数之后新向量Z中每一项表示每个类别概率值，即置信度，K是向量Z的长度，即类别的数量
                  3.  Softmax函数通常用于神经网络的输出层(多数情况是全连接层)，特别是在处理多分类问题时。Softmax函数可以将一组实数转换为**概率分布**，使得每一个数都在0到1之间，并且所有数的和为1。这样，每个数就可以被解释为属于某个类别的概率，最后这个包含该物体对每个类别分别概率的向量被传入交叉熵损失函数(分类问题),至此打通所有前向传播，后续开始反向传播和梯度下降
                  4.  卷积分类任务训练的目的，就是将该样本对应于其实际类别的得分在其对于所有类别的得分中的占比提到最大，也就是使Softmax概率预测函数得出的概率提到100%。

       - CNN核心算法：
          1. 梯度下降算法：
             1. 为什么会需要这种算法：
                - 模型训练的过程其实就是网络自行拟合原函数的过程，用众多个样本点作为实际值，每一层卷积核中所进行的仿射变换加上非线性激活层的非线性变换，到最后的全连接层时就会出来一个非线性拟合函数，用损失函数通过计算当前拟合的值和每个样本点的实际值之差等手段，衡量当前所拟合函数的质量。神经网络的目的，就是在原始数据集的每一轮训练后不断改变每个卷积核中权重和偏置这两个参数，得到一组最优的参数使得最后的损失函数的值最小，即拟合的效果最佳，而梯度下降算法就是让权重和偏置参数能够朝着让损失函数值变小的方向改变的一种手段。
             2. 损失函数：
                - 衡量预测值和真实值之间差异情况的函数
                - 种类：
                  1. **最小绝对误差(曼哈顿损失)(L1损失):**
                     - 公式：$$L1=\sum_{i=1}^n |Y_i-\hat{Y_i}|$$
                     - 特点：对所有大小的误差都给予相同的权重，对异常值不敏感
                  2. **均方误差(MSE损失函数)(L2损失)：**
                     - 通常用于回归问题，或者一些需要预测连续值的问题，如图像重建和超分辨率问题
                     -  公式：$$MSE=\frac{1}{n}\sum\limits_{i=1}^n(Y_i-\hat{Y_i})^2$$
                     -  特点：因为对损失值加平方，所以对更大的误差赋予更大的损失，意味着模型更加倾向于避免大的误差，对异常值更敏感
                  3. **交叉熵损失（CE损失函数）**
                     1. 此损失函数主要用于目标分类问题
                     2. 拟合的目标结果在于让交叉熵损失函数取得最小值，即让样本属于其真实类的概率最大
                     3. 在二分类和多分类问题中，使用one-hot编码来表示每个样本的真实标签，例如，如果有一个四分类问题，类别为 {A, B, C, D}，那么类别 A 可以被编码为 [1, 0, 0, 0]，类别 B 可以被编码为 [0, 1, 0, 0]，以此类推
                     4. **二分类问题：**
                        - $$CE=-\frac{1}{n}\sum\limits_{i=1}^{n}[Y_iln(\hat{Y_i})+(1-Y_i)ln(1-\hat{Y_i})]$$
                        - $Y_i$ 表示第i个样品是不是于这个类，是为1，不是为0，$\hat{Y_i}$ 表示属于这个类的预测概率
                        - 二元交叉熵函数前通常跟sigmoid函数，来将类别分数转化为0~1之间的概率
                        - 二元交叉熵函数同样能用来处理多类别分类问题，即，将一个N类分类问题视作N个二分类问题，在每一个类别的判断中都使用一个二元交叉熵损失函数，这样每个类别的概率互不影响，意味着可以做对一个目标的多分类问题，即，一个目标可以同时属于很多个类别。
                     5. **多分类问题：**
                        - $$CE=-\frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{K}Y_{ij}\ln(\hat{Y_{ij}})$$
                        - $Y_{ij}$ 是第i个样品标签中属于第j类的编码，是第j类就是1，不是就是0；$\hat{Y_{ij}}$ 是第i个样品的第j类的预测概率，由**Softmax函数输出的Z向量**(牛逼)得出，K是类别数量，n是样本数 
                        - 注意这里的样本数n表示，对于输入层输入的是很多张图片、甚至是不同类图片的情况（一个batch），最后的损失值是将的所有图片的损失值进行求和
                        - 注意：多元交叉熵损失函数只适用于对一个类别进行预测，他假设每个样本或检测框只属于一个类别。原因是多元交叉熵函数之前通常会搭配softmax函数，这意味着一个类别的分数会影响到其他所有类别的分数，所以各个类别的判断之间并不互相独立，所以不能用来对一个目标做多类别预测。
             3. 梯度下降算法：
                1. 概念：
                   - 对于非线性函数 $y=f(x,w,b)$ ,其中w、b分别是权重和偏置，在此处共同影响函数走势，神经网络的目的就是通过学习获得一组最优的w、b值，使得损失函数取得最小值，而对于损失函数 $L(w,b)$来说，其有两个自变量w，b，是一个空间中的二元函数，对于多元函数来说，**负梯度方向**是函数值减小最快的方向，为了让损失函数值以最快速度下降，应当让w、b沿着各自负梯度方向减小，所以每次得出损失函数之后应当按照负梯度方向更新w、b,其中二元函数的梯度向量可以表示为：$∇L = [\frac{∂L}{∂w}, \frac{∂L}{∂b}]$ ，$L()$ 关于w、b的的梯度分别是 $\frac{∂L}{∂w}$、$\frac{∂L}{∂b}$ ,以下均用 $g$ 表示。
                2. 公式：
                   - $$w = w - α * g$$
                   - $$b = b - α * g$$
                   - 这里的$\alpha$控制梯度下降的步长，是学习率
                3. 针对遇到的问题在原有梯度下降算法算法上做出的改进：**其实就是解决梯度下降算法中每一轮$w$、$b$减多少的问题**
                   1. 内存开销问题：
                      - 更新一次w、b值，需要计算出当前所有样本点的预测值，并交由损失函数计算，如果遇到样本点数量极多的情况，储存这些计算结果需要很大的内存开销，
                      - 解决方法：
                        - 算法：**随机梯度下降**:所以针对所有样本点，每次更新只随机且不重复地使用当中的一部分样本点，这样w、b依然可以沿着正确的方向收敛，同时加快计算速度，减轻内存负担
                   2. 损失函数震荡问题：
                      - 损失函数在梯度优化时，参数点可能在一个"山谷"的两侧来回震荡，即每次的梯度优化过于激进，不够平滑，导致损失函数迟迟无法掉入山谷，或者学习率控制的步长没有在后期需要细致优化时变得更小，始终保持不变，造成震荡，无法收敛
                        1. 从梯度角度解决：
                           - 算法：**动量随机梯度下降**：引入上一次的梯度作为“动量”，与这一次的梯度方向做矢量和成为新的梯度方向，这样可以使得每次的优化更加平滑
                           - 公式：
                             - $$v=\beta v-\alpha g$$
                             - $$w=w+v$$
                             - $$b=b+v$$
                             - 其中$v$是新引入的动量，初值是0，$\beta$是动量因子，控制对历史动量的保留程度，是超参数
                             - 此处的动量和一阶矩还是有区别的，动量是是过去梯度的累积，用于帮助优化算法在梯度方向上保持稳定的移动；一阶矩是过去梯度的指数移动平均值，用于平滑优化过程
                        2. 从学习率角度解决：
                           - 网络优化初期，梯度调节需要快速进行，可以大大提高效率，但是后期要想成功收敛，梯度调节的步长必须根据梯度大小等因素自行调节，梯度小时进行细致优化
                           1.  算法1:**Adagrad算法**
                                 - 公式：
                                   - $$r=r+g^2$$
                                   - $$w=w-\frac{\alpha}{\sqrt{r}+\delta}g$$
                                   - $$b=b-\frac{\alpha}{\sqrt{r}+\delta}g$$
                                   - 其中$r$是引入的梯度大小随时间的积累量，即**二阶矩**，在此处使用了指数移动平均(EMA)算法计算二阶矩，即近期数据的权重永远大于早期数据的权重，而且随着新数据到来随时更新，最后也不计算平均值，初值为0，$\delta$是为了防止分母为零引入的极小量
                                 - 优点：如果一个参数的梯度经常很大，那么$r$的值会变大，导致这个参数的学习率下降；反之，如果一个参数的梯度经常很小，那么$r$的值会变小，导致这个参数的学习率上升。这样就可以实现对每个参数的学习率进行自适应调整。
                                 - 不足：就是由于$r$是一直累积梯度平方的，所以在训练后期，$r$的值可能会变得很大，导致学习率过小，进一步导致训练过早停止。
                           2. 算法2:**RMSprop算法** 
                                - 公式：
                                  - $$r=\rho r+(1-\rho)g^2$$
                                  - $$w=w-\frac{\alpha}{\sqrt{r}+\delta}g$$
                                  - $$b=b-\frac{\alpha}{\sqrt{r}+\delta}g$$
                                  - $\rho$：衰减率，其为超参数，训练过程中不可人为改变，通常值为0.9或0.99，确定$\rho$ 需要通过交叉验证
                                  - $r$ ：同样是二阶矩，代表了梯度随时间积累的量
                                - 优点：其中相较于Adagrad算法优势体现在加入了可以手动调节的$\rho$，通过不断减小过去数据在总二阶矩中所占权重，可以解决后期二阶矩不断增大导致的学习率过小的问题
                                - 不足：$r$ 初值过小，
                        3. 同时从梯度角度和学习率角度：
                           - **Adam算法**
                             - 公式：
                               - $$s=\rho _1s+(1-\rho _1)g$$
                               - $$\hat s=\frac{s}{1-{\rho _1}^t}$$
                               - $$r=\rho _2r+(1-\rho _2)g^2$$
                               - $$\hat r=\frac{r}{1-{\rho _2}^t}$$
                               - $$w=w-\frac{\alpha \hat s}{\sqrt{\hat r}+\delta}$$
                               - $$b=b-\frac{\alpha \hat s}{\sqrt{\hat r}+\delta}$$
                               - $s$ 是自适应动量，或者说是一阶矩，继承自动量随机梯度下降算法；$r$ 是引入的梯度随时间积累的量，即二阶矩继承自Adagrad算法；$\rho_1$、$\rho_2$ 是衰减率，继承自RMSprop算法；$\hat s$、$\hat r$ 分别是 $s$ 和$r$ 随迭代过程的修正值
                               - $\rho_1$、$\rho_2$ 衰减率一般设置为0.9或0.99，这表明二阶矩中越新的数据所占权重越大，因为新数据权重始终为(1-$\rho$),但是所有的老数据的权重随数据更新在逐渐减小
                               - 之所以创建$\hat s$、$\hat r$,是因为$s$、$r$ 初值较小，接近于0，不能使用，但是后续两值会回归正常，使用公式则可以将初期的$s$、$r$ 放大，之后随着数据的更新，迭代的进行，分母中的$1-{\rho}^t$ 将逐渐接近于1，即，后期$\hat s$、$\hat r$ 将逐渐等于于$s$、$r$，而此时$s$、$r$的值也随着一阶、二阶矩的积累过程回归正常。
          2. 反向传播算法：(BP算法)
             1. 概念：神经网络中加速计算参数梯度值的方法
             2. 一般的计算各个参数梯度值的方法：
                - **前向传播过程**$$x >>> (w_1,b_1)>>>y_1=w_1x+b_1>>>(w_2,b_2)>>>y_2=w_2y_1+b_2>>>L(y_2,y_{gt})$$ 
                  - 其中，$(w_1,b_1)$、$(w_2,b_2)$ 分别是前后两次线性运算的参数，最后损失函数$L(y_2,y_{gt})$ 中 $y_{gt}$ 是真实值，$y$是计算值
                - **反向传播过程**$$\frac{∂L}{∂b_1}=\frac{∂L}{∂y_2}\frac{∂y_2}{∂y_1}\frac{∂y_1}{∂b_1}=\frac{∂L}{∂y_2}w_2<<<\frac{∂L}{∂w_1}=\frac{∂L}{∂y_2}\frac{∂y_2}{∂y_1}\frac{∂y_1}{∂w_1}=\frac{∂L}{∂y_2}w_2x<<<\frac{∂L}{∂b_2}=\frac{∂L}{∂y_2}\frac{∂y_2}{∂b_2}=\frac{∂L}{∂y_2}<<<\frac{∂L}{∂w_2}=\frac{∂L}{∂y_2}\frac{∂y_2}{∂w_2}=\frac{∂L}{∂y_2}y_1$$ 
                  - 其中每一步需要的量都由前向传播过程已知或由上一步已知
             3. 计算机中模块化加速计算梯度值方法
                - **前向传播过程**$$(x,w_1)>>>u_1=w_1x>>>(u_1,b_1)>>>y_1=u_1+b_1>>>(w_2,y_1)>>>u_2=w_2y_1>>>(u_2,b_2)>>y_2=u_2+b_2>>>L(y_2,y_{gt})$$
                  - 其中与一般计算不同的是这其中引入了单元运算概念，即将所有复杂运算拆解成一个个的简单、步骤高度重合、只是数值发生改变的运算，这样就可以将正向传播和反向传播过程用程序来实现
                - **反向传播过程**$$\frac{∂L}{∂w_1}=\frac{∂L}{∂y_2}\frac{∂y_2}{∂u_2}\frac{∂u_2}{∂y_1}\frac{∂y_1}{∂u_1}\frac{∂u_1}{∂w_1}=\frac{∂L}{∂y_2}w_2x<<<\frac{∂L}{∂b_1}=\frac{∂L}{∂y_2}\frac{∂y_2}{∂u_2}\frac{∂u_2}{∂y_1}\frac{∂y_1}{∂b_1}=\frac{∂L}{∂y_2}w_2<<<\frac{∂L}{∂w_2}=\frac{∂L}{∂y_2}\frac{∂y_2}{∂u_2}\frac{∂u_2}{∂w_2}=\frac{∂L}{∂y_2}y_1<<<\frac{∂L}{∂b_2}=\frac{∂L}{∂y_2}\frac{∂y_2}{∂b_2}=\frac{∂L}{∂y_2}$$
                  - 其中每一步需要的量都由前向传播过程已知或由上一步已知
                - 计算机模块化和计算方法深度学习框架相配合，在每个单元运算中均有定义的正向传播和反向传播函数，给每个神经元之间创造了联系通路，神经元拥有了生命。
          3. 神经网络训练的过程，其实就是**前向传播计算数值**>>>**反向传播算法获取梯度**>>>**梯度下降算法更新参数**


   2. 初识yolox（CSPDarknet、Darknet53，两者只在backbone部分有区别，其他部分基本一致）
      1. 训练过程的整体结构 ![yolox](picture/2.png)
         1. **输入端**
            1. 数据增强：
               1. **Mosaic数据增强**
                  - 大致过程：四张图片通过随机缩放、随机裁剪、随机排布的方式进行拼接，合成一张图片,这几张图片之间互相填充，填满整个输入图像的每个区域
                  - 优点：增加样本的多样性、提高模型对小而密集目标的识别能力、让模型在一个batch中处理更多张图片，提高GPU利用率
               2. **Mixup数据增强**
                  - 大致过程：将两张图片上下左右填充之后缩放到输入图片标准大小，然后设置一个叠加系数将两张图片上下叠加，相互融合，使得两张图片中的元素包含检测框也会相互重合
                  - 优势：增加样本多样性、引入随机性防止过拟合、实现简单
               3. 需要注意：使用Mosaic和Mixup两种数据增强方式会让ImageNet预训练模型变得失去意义，因为所有数据集都得从头开始训练
            2. 切片下采样：
               1. 将每张图片复制为4份，分别经过四次不同的切片操作
                  -  **slice**：表示切片过程，主要有四个参数
                     1. **starts** ：表示切片起点，如果是[1,1],则表示从第二行第二列的元素开始切片
                     2. **ends**   ：表示切片的元素索引，如果是[9223372036854775807, 9223372036854775807]，这两个数是python中64位整数的最大值，表示切片到维度末尾
                     3. **axes**   ：表是切片的维度，对于四维张量而言，如果axes=[2,3],则表示在每张图片的高度、宽度上切片
                     4. **steps**  ：切片步长，如果是[2,2],则表示两个维度上的都是每两个取一个
               2. 切片之后对结果进行维度上的拼接，即进行concat操作 
         2. **Backbone**
            1. 作用：**提取图像特征**
            2. 构造:  ![Backbone](picture/3.jpeg)
                  1. 基于**CSPDarkNet**的Backbone:
                     1. 结构：
                        1. **CSP结构**：跨阶段部分连接层，思想来自于**CSPNet**，CSPDarkNet的骨架，是一种特殊的层结构，将来自网络某一阶段的特征图分为两部分。一部分直接传递到后续阶段，而另一部分经过额外的处理（如卷积层）后再与直接传递的部分合并，这种方法减少了冗余计算，因为并非所有特征都经过每个卷积层的处理。它更注重于**提高网络的训练效率和降低计算成本**，同时保持特征的多样性，  
                              1. **特征重塑结构**：CSPLayer内的基本结构，将卷积核、sigmoid激活函数、乘法操作结合在一起，即卷积核输出的特征图先通过一次sigmoid函数将特征值转换为权重系数，然后将其与特征图逐元素相乘，得到最后的新特征图，这可以改变特征的分布或重要性，帮助模型专注于提取更重要的特征
                              2. **Bottleneck结构**：CSPLayer中的标准神经网络构构件，此处是实现了残差连接功能的Bottleneck结构
                                 - 相比于CSPLayer中的其他构件，BottleNeck主要负责实现残差连接功能，与**Resn**相同
                        2. **SPP结构** :空间金字塔池化(正常来说SPP的输出是一个固定长度的向量，但是yolox中略有不同，还是保留了其二维矩阵形状，因为空间位置信息对于目标定位非常重要，除此之外，其还使用外围填充和设置互动窗口步长为1的方案使得输出特征图尺寸和输入特征图相等)，思想来自于**SPPNet** 
                           1. 作用：提取多尺度特征
                           2. 作用过程：输入特征图上进行不同尺度的最大池化操作，然后将这些不同尺度的特征图和**输入的特征图**拼接在一起，从而得到一组丰富的、多尺度的特征，其中在拼接之前，需要将所有特征图的尺寸调整为相同大小，可以使用类似于上采样操作中的最近邻插值、双线性插值等方法，最后是**在通道维度上相加(concatenation)**，也就是沿着通道轴堆叠起来。
                  2. 基于**DarkNet53**的Backbone：
                        1. **CBL**：是一个常见层组合，即*Convolution-BatchNorm-LeakyReLU*，是将卷积层、批量归一化层、LeakyReLU激活层组合在一起使用，在YOLOXDarknet53中使用广泛
                        2. **Resn**:残差网络**ResNet**中引入的一种残差连接，由残差块组成，Resn就表示这个残差连接中包含n个残差块，相互串联连接，其中每个残差块通常结构如下：
                           - **卷积层**(降低维度,残差单元的第一层，通常使用1x1的卷积核，步长为1)
                           - **激活层**(引入非线性,ReLU激活函数)
                           - **卷积层**(提取特征,接下来是一个3x3的卷积层，步长为1)
                           - **激活层**(再次使用ReLU激活函数)
                           - **卷积层**(恢复维度,最后是一个1x1的卷积层，步长为1)
                           - **跳跃连接**(直接将最初的输入信号传递到最后一个卷积层的输出，并**逐元素相加**，如果输入的特征图大小和经过最后一个卷积层出来的特征图的尺寸或深度不一致，则会使用步长为一定值的1*1卷积核将输入的特征图的尺寸和深度调整至于输出特征图一直，然后进行逐元素相加)
                           - **激活层**(相加结果再通过ReLU激活函数，得到残差单元的最终输出)
                        3. **SPP** ：同CSPDarkNet
         3. **Neck**![Neck](picture/4.jpg)
            1. 作用：特征融合和特征选择
               1. 基于**FPN****结构的Neck结构：
                     1. 结构思想：使用的是**特征金字塔网络（FPN）**中的结构，FPN主要思想是构建一个多尺度融合金字塔，同时处理不同尺度目标，主要组成部分：**自底向上的常规卷积网络结构**、**自顶向下的上采样(只改变大小)和1*1卷积核操作(只改变维度)**
                        1. **上采样**是一种增大特征图尺寸的操作，常见的上采样方法有最近邻插值、双线性插值等。在FPN中，上采样操作通常用于将高层（尺寸较小，**语义信息丰富**）的特征图尺寸放大，以便与低层（尺寸较大，**位置信息丰富**）的特征图进行融合。
                        2. 1*1卷积核操作则通过卷积核数量改变特征图的维度
                        3. **下采样**：是一种减少数据量的操作，通常指减少图像尺寸的过程，在CNN中，卷积层、池化层中均有下采样操作
                        4. 对于每一层，由前向传播过程得到的每张特征图会与上采样操作得到的特征图进行**维度上的相加**，得到新的特征图继续向下进行上采样操作，这样的构造会使得新的特征图即具有高层的语义信息还具有底层的位置信息，具有更好表达能力
                        5. **concat**：上采样中常用的融合结构，将两种特征图进行**维度**上的拼接
                     2. 将来自Backbone的三个层级的特征图进行融合之后再输出给Prediction结构
               2. 基于**PAN**结构的Neck结构：
                     1. 在FPN的基础上增加了从下向上的路径，即经过FPN操作后再依次向上层特征做融合，这有助于将底层细节信息更好地传递给高层次，改善模型对小目标的检测能力
               3. 基于**GhostPAN**结构的Neck结构：
                     1. 其结构思想来源于传统PAN，但是引入了**GhostBottleNeck**，使用更少的卷积操作来生成更多特征图，减少了计算量和参数量；还引入了**深度可分离卷积层**，使用深度卷积处理每个输入通道，再使用逐点卷积(1*1卷积)将结果合并，同昂减少了计算量和参数量
         4. **Prediction** 处理预测框信息、损失函数计算、输出
            1. **Decoupled Head(解耦头)**![解耦头](picture/1.png)
               1. 作用：生成预测框，进行一系列参数预测
               2. 构造：
                  1. 输入特征图的尺寸由来：yolox中巧妙地将Backbone结构中对原始图片的下采样操作数带入了Pridiction结构，由于每次下采样操作都是将原特征图边长减半的过程，Neck中的三个输出分支分别对应Backbone层中的5次、4次、3次下采样操作，所以三个解耦头的输入的特征图边长分别是原始图片边长的32分之一、16分之一、8分之一，又因为**解耦头输入的特征图其实就是对原始图像进行完全、均匀划分(yolox默认输入图像大小为640x640，比如输入特征图尺寸为20x20，其实就是把原始图像均匀化分成了400个网格单元，每个网格单元负责预测原始图片中32x32大小的区域)，并分别预测。**所以三个解耦头中的每个网格在原始图片中对应的大小分别为32x32、16x16、8x8
                  2. 由Neck层过来的三个分支分别连接不同尺寸的解耦头(20x20、40x40、80x80)，在单个解耦头里面，又总共包含三个分支，**分别对不同参数进行独立学习**
                     1. 其中两个在CBL、单卷积层、sigmoid激活函数层作用下分别进行**类别预测**(对于N个类别的分类，使用N个二分类实现)和**前景预测**(预测框中物体是否为感兴趣物体)，其输出的通道数分别等于类别数和1
                     2. 还有一个分支直接进行**坐标信息预测**(预测框中心点的x、y坐标以及预测框的长度宽度w、h)，其生成特征图的通道数等于4
                     3. 三个分支分别生成大小相同，通道数不同的一系列特征图，在**concat**中进行深度上的拼接，最终变成一个通道数=(类别数+1+4)的特征图，这张特征图的尺寸表示这个解耦头一共生成了多少个预测框(一个解耦头在一个网格单元中生成一个预测框)，特征图的深度对应每个预测框的每个参数信息。
               3.  在解耦头之后，每个输出经过 ![解耦头之后](picture/5.jpg) 
                   1. **Reshape操作**从三维张量变成二维矩阵，其中行数等于预测框数(即张量的尺寸大小)，列数等于总参数数(原张量的深度)，
                   2. **concat**操作将三个解耦头的输出矩阵拼接在一起，产生一个8400行的矩阵，代表三个解耦头一共产生8400个预测框。
                   3. **transpose**操作，将二维矩阵转置，这时每一列代表一个检测框，一共有8400列，每一行都代表一个参数信息
            2. **Anchor Free**：不适用预先定义好的检测框，直接从物体特征到物体边界框进行端到端学习
               1. 传统方式**Anchor Based**：训练过程依赖最初定义好位置和大小的预测框，之后的学习过程不断调整最初定义好的框的大小和位置以实现匹配图像中的目标，计算过程较为复杂，但是准确度更高
               2. Anchor Free则不需要预先定义好的预测框，直接对预测框的中心点坐标、长宽进行回归，端到端方式，识别更快
            3. **标签分配**：在所有预测框中挑选出合适的正预测框
               1. 初步筛选：主要有两种方式
                  1. 根据预测框中心点坐标判断：
                     -  在yolo系列的标注文件中，通常包含真值框的左上角、右下角的坐标信息(x1,y1)、(x2,y2)，将其与每个预测框的中心点坐标信息(x0,y0)进行比对，如果x1≤x0≤x2、y1≤y0≤y2,即可判断出此时预测框中心点坐标在真值框内，并将其作为初步筛选出的预测框
                  2. 根据真值框来判断：
                     - 以真值框中心点为基准，设置边长为5的正方形，挑选中心点落在正方形内的所有预测框
               2. 精细化筛选：需要经过如下几步：
                  1. **loss函数计算**：
                     1. **定位损失**：将每个真值框和对应的预测框之间的**iou信息**(用来衡量两个矩形区域之间重叠程度，即交集面积比上并集面积，值在0~1之间，越接近1表示重叠度越高)通过-torch.log(就是-log函数)转化为位置损失，即当iou值接近于1时，损失值接近于0，当iou值在0~1之间远离1时，损失值迅速增大。
                     2. **类别损失**：将每个类别的条件概率(即类别预测结果)和目标的先验概率(训练集中该类别出现的概率)做乘积，得到目标的类别分数，再经过二元交叉熵损失函数得到每个类别的损失值，然后求和得到总的类别损失值
                  2. **cost成本计算**：
                     1. 公式：$$c_{ij}=L_{ij}^{cls}*\lambda L_{ij}^{reg}$$
                        1. $c_{ij}$：每个预测框最终的cost成本
                        2. $L_{ij}^{cls}$：类别损失
                        3. $L_{ij}^{reg}$：定位损失
                        4. $\lambda$：加权系数
                  3. **SimOTA** ：为每个真值框分配最终的预测框
                     1. **OTA问题**：目标检测领域中是指找到一种最优的方式将检测框与真值框匹配起来，其核心在于如何有效、准确地分配预测框到真值框，以最大化整体的检测性能，为减轻计算负担、提高匹配准确性，诞生了SimOTA
                     2. 具体步骤：
                        1. 确定给每个真值框分配预测框的数量
                           1. 设置候选预测框数量：给每个真值框先选择10个iou信息最高的预测框
                           2. cost挑选候选预测框：将每个真值框挑选出来的10个预测框的cost值求和，并取整，最后得到的数就是给这个真值框分配的预测框数量
                        2. 选择对应数量的预测框
                           1. 在目前为止挑选出来的所有检测框中，将每个检测框对于每个真值框的cost成本都排列出来，形成一个列表，行是真值框，列是检测框，将每个真值框那一行按照其被分配的名额，挑选相应数量的cost成本最低的检测框，如果遇到有一个检测框同时被多个真值框选择的情况，则只将此目标框给对应cost值最小的真值框。
            4. **loss计算**：
               1. 选择好每个真值框对应的预测框之后，就可以开始进行损失函数计算了，此处与筛选预测框时使用的损失函数不同
                  1. **定位损失**可以使用iou_loss、giou_loss、L1损失三种损失函数计算方式，实际使用时最后的定位损失还可能会乘以一个权重系数
                     1. $iouloss=1-iou$
                     2. $giouloss=1-(iou- \frac{C-A\bigcup B}{C})$
                        1. A、B:预测框和真值框
                        2. C:   可以包含A、B的最小封闭框
                  2. **类别损失**则是BCE_loss，即二元交叉熵损失函数计算
                  3. **前景损失**是将预测框挑选之前生成的所有预测框的损失之和，同样是BCE_loss，最后将所有预测框对应的损失求和，进行梯度下降和反向传播算法即可进行学习
      2. 推理过程的筛选
         1. **置信度筛选**：经过训练之后，yolox模型在进行推理时可以在目标周围生成很多预测框，得出最终的推理结果只需要保留预测结果最好的几个预测框，此处使用**置信度**来衡量一个预测框的预测结果好坏，其由每个预测框的最高类别得分乘以前景得分计算而来，在推理前可以预先设定置信度阈值，即最后只会保留高于这个阈值的检测框
         2. **NMS筛选**：对于保留下来的置信度较高的预测框，很显然不能全部保留，因为可能会有很多预测框几乎是重叠的，框选的是同一个目标；但是也不能只保留置信度最高的那个预测框，因为如果遇到多个检测目标排列很紧密的情况，只保留一个预测框意味着同一类别的目标将有大量被遗漏，所以NMS算法，即非极大值抑制（Non-Maximum Suppression）算法用于只去除同一类别中重叠度很高的一系列检测框，其原理如下：将同一类别所有保留下来的检测框按照置信度排序，取置信度最高的一个检测框为基准框，计算其与其他检测框的IOU值，去除掉IOU值大于**阈值**(超参数，由命令行输入)的所有框，保留剩下的重叠度不高的检测框，其可能是识别到了同一类别的不同对象
         3. 综上：置信度和NMS的两个阈值共同对检测框进行筛选。

5. **GAN**生成对抗网络：
   1. 初步理解：生成对抗网络核心思想是**二人零和博弈**，即**生成器**和**判别器**通过不断对抗来提高自身性能，最终达到纳什均衡
      1. 生成器：
         1. 所做的事：根据所给的随机数生成图片
         2. 目标：使得生成的图片越来越真，直到骗过判别器，让其无法分辨真假
         3. 学习源：判别器
      2. 判别器：
         1. 所做的事：对来自生成器的图片进行判断，辨别其是否是真实图片
         2. 目标：增强对真假图片的判别能力，直到能够准确无误的将所有来自生成器的图片都判别为假
         3. 学习源：训练时所给的数据集和生成器生成的图像
   2. 几种生成对抗网络的结构
      1. 基于**全连接网络**的最初等GAN
      2. 基于生成式对抗网络的超分辨率重建模型**SRGAN**
         1. **生成器结构**：
            1. 
         2. **对抗器结构**：
            1. 